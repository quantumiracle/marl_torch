import gym
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical
import argparse
import numpy as np
import os
import copy
import time
import pettingzoo
from utils.wrappers import PettingZooWrapper, make_env
from utils.ppo import PPODiscrete, MultiPPODiscrete
from hyperparams import *

def parallel_rollout(env, model, exploiter, max_eps, max_timesteps, selfplay_interval, render,\
     model_path, exploiter_path, against_baseline=False):
    score = {a:0.0 for a in env.agents}
    avg_score = []
    epi_len = []
    print_interval = 20
    save_interval = 100
    [n0, n1] = env.agents

    for n_epi in range(max_eps):
        # env.seed(np.random.randint(1000))  # take random seed for evaluation
        observations = env.reset()

        for t in range(max_timesteps):
            m_actions, _ = model.choose_action(observations)
            e_actions, logprobs = exploiter.choose_action(observations)
            actions = {n0: e_actions[n0], n1: m_actions[n1]}
            try:  # only for slimevolley
                observations_, rewards, dones, infos = env.step(actions, against_baseline)  # from discrete to multibinary action
            except:
                observations_, rewards, dones, infos = env.step(actions) 
            if render:
                env.render()
                # time.sleep(0.1)
            
            exploiter.put_data((observations, e_actions, rewards, observations_, logprobs, dones))

            observations = observations_

            for agent_name in env.agents:
                score[agent_name] += rewards[agent_name]

            if np.any(np.array(list(dones.values()))):  # any agent has a done -> terminate episode
                break
            # if not env.agents: # according to official docu (https://www.pettingzoo.ml/api), single agent will be removed if it recieved done, while others remain 
            #     break 

        exploiter.train_net()
        epi_len.append(t)
        # record training info
        if n_epi % print_interval == 0:
            print("# of episode :{}".format(n_epi))
            avg_score = score[n0] / float(print_interval)  # the score of the exploiter
            avg_len = int(np.mean(epi_len))
            print(f"exploiter avg. score: {avg_score}, avg. epi length: {avg_len}")
            score = {a: 0.0 for a in env.agents}
            epi_len = []
            if n_epi % save_interval == 0 and n_epi != 0:
                exploiter.save_model(exploiter_path + 'mappo_single')
    print(f"Final score: {avg_score}, episode length: {avg_len}")
    return avg_score, avg_len

def main():
    parser = argparse.ArgumentParser(description='Train or test arguments.')
    parser.add_argument('--env', type=str, help='Environment', required=True)
    parser.add_argument('--ram', dest='ram_obs', action='store_true', default=False)
    parser.add_argument('--render', dest='render', action='store_true',
                    help='Enable openai gym real-time rendering')
    parser.add_argument('--seed', dest='seed', type=int, default=1234,
            help='Random seed')
    parser.add_argument('--load_agent', dest='load_agent', type=str, default=None, help='Load agent models by specifying: 1, 2, or both')
    parser.add_argument('--against_baseline', dest='against_baseline', action='store_true', default=False)
    parser.add_argument('--fictitious', dest='fictitious', action='store_true', default=False)        
    args = parser.parse_args()

    if args.ram_obs or args.env == "slimevolley_v0":
        obs_type='ram'
    else:
        obs_type='rgb_image'
    env = make_env(args.env, args.seed, obs_type=obs_type)
    exploit_eps = 1000

    state_spaces = env.observation_spaces
    action_spaces = env.action_spaces
    print('state_spaces: ', state_spaces, ',  action_spaces: ', action_spaces)

    device_idx = 0
    device = torch.device("cuda:" + str(device_idx) if torch.cuda.is_available() else "cpu")
    learner_args = {'device':  device}
    env.reset()
    print(env.agents)
    agents = env.agents

    fixed_agents = ['second_0']  # both the model and exploiter fix the second agent, so the first agent of exploiter can learn 

    if obs_type=='ram':
        model = MultiPPODiscrete(agents, state_spaces, action_spaces, 'MLP', fixed_agents, learner_args, **hyperparams).to(device)
        exploiter = copy.deepcopy(model)
    else:
        model = MultiPPODiscrete(agents, state_spaces, action_spaces, 'CNN', fixed_agents, learner_args, **hyperparams).to(device)
        exploiter = copy.deepcopy(model)

    if args.fictitious:
        model_dir = 'model/{}/fictitious_selfplay/'.format(args.env)
        exploiter_dir = 'model/{}/fictitious_selfplay/exploiter/'.format(args.env)
    else:
        model_dir = 'model/{}/selfplay/'.format(args.env)
        exploiter_dir = 'model/{}/selfplay/exploiter/'.format(args.env)
    os.makedirs(model_dir, exist_ok=True)
    os.makedirs(exploiter_dir, exist_ok=True)

    filelist, epi_list = [], []
    for filename in os.listdir(model_dir):
        if filename.endswith("policy"):
            filelist.append('_'.join(filename.split('_')[:-1]))  # remove '_policy' at end
            epi_list.append(int(filename.split('mappo')[0]))
    sort_idx = np.argsort(epi_list).tolist()
    filelist = [x for _,x in sorted(zip(epi_list,filelist))] # sort filelist according to the sorting of epi_list
    epi_list.sort()  # filelist.sort() will not give correct answer   
    print(epi_list)

    r_list, l_list = [], []
    eval_data={}
    for f, i in zip(filelist, epi_list):
        print('load model: ', i, f)
        # if i>17000: 
        print(model_dir+f)
        model.load_model(agent_name=fixed_agents[0], path=model_dir+f)
        exploiter_path = exploiter_dir+f

        r, l = parallel_rollout(env, model, exploiter, max_eps=exploit_eps, max_timesteps=max_timesteps, selfplay_interval=selfplay_interval,\
            render=args.render, model_path=None, exploiter_path=exploiter_path, against_baseline=args.against_baseline)
        eval_data[str(i)]=[r, l]
    save_dir = 'data/{}/'.format(args.env)
    os.makedirs(save_dir, exist_ok=True)
    if args.fictitious:
        save_dir+='/fictitious_eval_data.npy'
    else:
        save_dir+='/eval_data.npy'
    np.save(save_dir, eval_data)

    env.close()

if __name__ == '__main__':
    main()
